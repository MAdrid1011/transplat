## 可泛化3DGS Encoder设计挑战

### Challenge 1：几何引导的跨视图采样导致有效带宽利用率受限

在前馈 3DGS 生成式 Encoder 中，跨视图特征采样是建立多视图几何对应关系的核心算子。现有方法主要采用两种几何引导的采样策略：PixelSplat 对每个 query 像素在参考视图上沿极线采样一组像素位置 $\{\tilde{u}_l\}$，并通过 query 像素 $u$ 与采样点 $\tilde{u}_l$ 的三角化得到每个 sample 的深度，用于 depth positional encoding 后执行 epipolar cross-attention；TransPlat 则采用 plane-sweep 式重投影采样，参考采样位置由相机投影矩阵 $P_i, P_j$ 与深度候选 $d^{\text{cand}}$（D=128 个离散深度）生成，并在 Deformable Sampling 中通过可学习偏移（每个参考位置 P=4 个偏移点）进一步聚合局部邻域信息。尽管两者在具体实现上存在差异，但其访存位置均由相机内外参主导，而非规则的二维网格，因此统称为几何引导采样（Geometry-Guided Sampling）。

与传统卷积或规则网格采样不同，几何引导采样的访存地址分布呈现显著的不规则性：不同 query 对应的采样区域在特征图上高度分散，即便同一 query 的多个采样点也可能跨越不连续的内存区域。这种访存模式与 GPU 缓存系统的设计假设存在根本性矛盾——后者依赖空间局部性进行预取和缓存行复用，而几何引导采样的访存轨迹由相机位姿决定，无法被通用缓存策略有效捕获。

我们在 NVIDIA H100（80GB）GPU 上对两种方法进行了详细 profiling。TransPlat（特征图 32×32，D=128，P=4）的带宽利用率仅为 38.0%，其采样点因可学习偏移无法保证空间连续性；PixelSplat（特征图 64×64，沿极线采样 32 点）尽管采样轨迹完全可预测，但因采样点稀疏、单次访存数据块小，带宽利用率更低至 1.1%。两者均远低于 H100 HBM 峰值带宽 3958 GB/s 的理论上限。为验证几何结构本身是否提供局部性，我们进行了对比实验：在相同采样规模下，完全随机采样耗时 1.279 ms，而沿极线方向采样降至 0.637 ms（2.01× 加速），说明极线几何确实提供了一定的空间局部性；但该性能仍显著低于理论连续访存上限（约 0.25 ms），存在约 2.6× 的差距。

Roofline 分析进一步揭示了瓶颈本质：该算子的算术强度仅为 0.438 FLOP/Byte，远低于 H100 的带宽-计算拐点（249.9 FLOP/Byte），性能完全受限于内存系统。根本原因在于：GPU 的通用缓存替换与预取机制无法感知几何引导采样的空间分布规律——相邻 query 虽有相近的采样区域，但采样轨迹在特征图上的平移与偏移导致缓存行在被充分复用前即被逐出；双线性插值引入的四邻域访问进一步加剧了缓存行碎片化。上述因素共同导致几何引导采样在 GPU 上长期处于低带宽利用率状态，构成前馈 3DGS Encoder 的首要性能瓶颈。

### Challenge 2：采样–聚合的串行依赖引发中间数据显式放大

除访存效率外，几何引导采样在算法结构上还引入了显著的中间数据放大问题。现有前馈 3DGS 方法普遍采用"先采样、后聚合"的执行范式，即必须在完成所有采样点的特征采样之后，才能进行加权聚合。这种严格的阶段性依赖迫使中间采样结果以显式张量形式存储于 HBM 中。

对两种方法的实测数据表明：TransPlat 对每个 query 在 D=128 个深度候选位置执行采样，并在每个深度位置通过 Deformable Sampling 聚合 P=4 个偏移点，总计 512 个采样值，中间张量占用约 512 MB 显存；PixelSplat 沿极线采样的点数较少，但由于特征图分辨率较高，其中间张量实测占用约 256 MB。尽管具体形状和放大程度不同，但两者都存在显著的中间数据显式物化问题：采样结果必须先写入 HBM，随后才能被聚合阶段读取。

以 TransPlat 为例的阶段级 profiling 显示，几何引导采样（grid_sample）阶段耗时 5.42 ms（56.5%），并将中间数据写回 HBM；随后的加权聚合阶段耗时 3.90 ms（40.7%），需要再次从 HBM 读取同一中间张量，两阶段合计占据 97.2% 的执行时间。对比实验表明，当通过 kernel 融合消除中间张量的显式写回后，执行时间从 9.32 ms 降至 4.35 ms（2.14×），验证了中间数据往返是主要瓶颈来源。然而，即便在融合执行的情况下，几何引导采样仍需以不规则模式访问特征图，其内存受限本质并未改变，从而限制了 GPU 侧优化的整体上限。

### 我们的方法

为解决上述两类瓶颈，我们提出 GeoPIM——一种面向前馈 3DGS Encoder 几何引导采样的存内加速架构。该架构通过算法数据流重构与存内硬件协同设计，系统性地缓解带宽利用率受限与中间数据放大问题：

#### Method 1：几何参数驱动的语义感知预取

为应对 Challenge 1 中的低带宽利用率问题，GeoPIM 在 HBM-PIM 侧部署几何参数驱动的语义感知预取机制。该机制利用几何引导采样的结构化特性——PixelSplat 的极线采样轨迹完全可预测，TransPlat 的可学习偏移在局部范围内——在存内侧维护几何参数寄存器，通过并行地址生成单元提前预测后续采样点的 HBM 地址，并在缓存管理中引入采样区域感知的保留策略，避免通用 LRU 下的过早驱逐。通过在内存侧显式编码几何语义，该方法将隐含于算法结构中的局部性转化为可被硬件利用的访存规律，从而显著提升带宽利用率。

#### Method 2：基于统一权重路径的流式采样–聚合融合

针对 Challenge 2 中的中间数据放大问题，GeoPIM 提出基于统一预计算权重的流式融合数据流。该方法的核心在于将权重计算与采样聚合解耦：对于权重与采样值解耦的聚合（如 TransPlat 的 Deformable Attention），权重由 query 特征直接预测，可在采样前完成；对于权重依赖采样值的聚合（如 PixelSplat 的 Cross-Attention），GPU 首先执行轻量级的 key 采样获取低维特征，计算 Q-K score 并完成 softmax 归一化后，将权重传递至 PIM。这一设计使 PIM 无需 exp/div 等复杂运算单元，满足 HBM-PIM 的门级约束（<10K gates/bank），同时确保所有方法都走"预计算权重 + 加权累加"的统一数据流。该方案在 GPU 上难以与几何引导采样高效融合，因为后者的不规则访存模式无法静态编排为规则 tiling；而 GeoPIM 通过将加权累加迁移至存内侧，使采样值在生成后即被消费，从根本上消除中间张量物化。

GeoPIM 的两项设计形成协同：Method 1 的语义感知预取为 Method 2 的流式消费提供数据就绪保障，使"预取-消费"流水线得以建立；Method 2 的流式融合则确保预取的数据被立即利用而非写回 HBM。该架构对 plane-sweep 重投影 + Deformable Attention（TransPlat）和极线采样 + Cross-Attention（PixelSplat）均适用——前者权重由 MLP 预测，后者权重由 GPU 完成 Q-K→softmax 后下发——验证了 GeoPIM 作为前馈 3DGS Encoder 通用加速器的泛化性。
